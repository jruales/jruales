{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data analysis to find the best Wordle starting word\n",
    "In chess, part of what makes good players good is that they've memorized openings. Because every chess game starts in the same position and the combinatorial explosion of possible positions doesn't happen until a few moves in, it's feasible to memorize the optimal starting few moves, regardless of what the opponent plays.\n",
    "\n",
    "If this is something that exists in chess, then would it be possible to do something similar in Wordle, to find an optimal or near-optimal starting move?\n",
    "\n",
    "tl;dr: use **\"soare\"** as your starting word. Read on to learn more about why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Wordle\n",
    "\n",
    "[Wordle](https://www.powerlanguage.co.uk/wordle/) is a word game where the objective is to guess a 5-letter word. The word to guess is different each day, and you have 6 guesses to get it right. Once you try guessing a word, the game marks each letter you entered in one of three possible colors: Green if the letter is in the solution word in the correct place, Yellow if the letter is in the solution word but in the incorrect place, and gray if it's not in the solution word.\n",
    "\n",
    "Here are the official instructions as mentioned in the game website:\n",
    "\n",
    "![Official instructions indicating the meaning of green, yellow, and gray tiles](img/official_instructions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A detail that was left out from the instructions\n",
    "\n",
    "Consider these attempts:\n",
    "\n",
    "![Game attempts that shows word Alaap has the first two A's gray and last one green. The word Araba shows with the first A yellow and the other two gray](img/yellow_green_mechanism_cropped.png)\n",
    "\n",
    "Even though we know that the letter A is part of the answer, not all A's in the attempts are colored, which seems to contradict the instructions, where it said that gray letters aren't part of the solution word.\n",
    "\n",
    "As it turns out, the colors can also tell us the **number of instances** of a specific letter in a solution. In the example above, the solution only has one A, so Wordle makes sure to only color as green or yellow a single instance of the letter A in any given guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In search of the best starting word\n",
    "Like in chess, each game of Wordle starts with the same starting position. At the start, you have no information about the word (other its length, which is always 5). This means that there must be an optimal first word that would, on average, maximize one's chance of guessing the word within the six allowed attempts, and that one could use every time. Everyone has their starting word or words that they have chosen based on anecdotal success or other heuristics, but I wanted to find a word that is objectively good according to a data-driven metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But first, we need a word list\n",
    "Wordle has its own dictionary of allowed words, and it doesn't accept any attempts of words that aren't in that list. The best starting word we're looking for needs to be within that dictionary or it won't be useful. One approach here would be to use another word list, such as the Scrabble dictionary, or some other 1-gram word corpus online. Then, after finding the best word, we could try this word in Wordle to ensure it was valid. But wouldn't it be better if we could simply have the list of words that Wordle checks against for validity? Maybe looking at the source code for the game will reveal the list...\n",
    "\n",
    "So I did Right Click -> Inspect Element and that's basically all it took to find the list.\n",
    "\n",
    "Under the Network tab in the Developer Tools, after refreshing the page, a file appears called something like \"main.e65ce0a5.js\" that contains the main code for the game. Under the Preview sub-tab, I searched for some five letter words, and lo and behold, there was the list.\n",
    "\n",
    "What was surprising, though, was that there were actually two word lists in the code right next to each other:\n",
    "\n",
    "![The source code showing two arrays of five-letter words](img/two_word_arrays.png)\n",
    "\n",
    "The code is obfuscated, so it's not possible to see what the variable names were originally. Searching for recent Wordle solutions such as \"tangy\" or \"abbey\", it's clear that past solutions are in the first array but not in the second. So it turns out that the first array, which has 2315 entries, contains solutions. The second array, with 10657 words, has all non-solution words that are valid guesses, sorted in alphabetical order.\n",
    "\n",
    "> Sidenote: Solutions are on the first array _in order_, so one only needs to keep on going along the list and one will find all past and future solutions. Now, there's no fun in just knowing the solution and winning in one move. I mean, if you wanted to do that, you could just look up the solution for the day online. Instead, let's keep on searching for the single best starting word.\n",
    "\n",
    "I grabbed the words from each array and transferred them to text files words_solutions.txt and words_other.txt respectively, with one word per line. Then I read them into a Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_solutions):\t 2315\n",
      "len(words_other):\t 10657\n",
      "len(words_all):\t\t 12972\n"
     ]
    }
   ],
   "source": [
    "with open(\"words_solutions.txt\", \"r\") as file:\n",
    "    words_solutions = {line.strip() for line in file}\n",
    "with open(\"words_other.txt\", \"r\") as file:\n",
    "    words_other = {line.strip() for line in file}\n",
    "words_all = words_solutions | words_other\n",
    "print(\"len(words_solutions):\\t\", len(words_solutions))\n",
    "print(\"len(words_other):\\t\", len(words_other))\n",
    "print(\"len(words_all):\\t\\t\", len(words_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helpers ###\n",
    "\n",
    "def avg(_list):\n",
    "    return sum(_list)/len(_list)\n",
    "\n",
    "# computes the average number of reduced words when using this word as a guess.\n",
    "def compute_score(guess_word, metric):\n",
    "    return avg([metric(guess_word, actual_word) for actual_word in list(words_solutions)])\n",
    "\n",
    "# Finds the word that maximizes the given metric. \n",
    "def find_optimal_word(metric, words_to_check=words_all):\n",
    "    max_score = -1\n",
    "    max_score_word = None\n",
    "    for guess_word in words_to_check:\n",
    "        score = compute_score(guess_word, metric)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_score_word = guess_word\n",
    "    return max_score_word, max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some metrics that measure how good a guess is\n",
    "We define six metrics:\n",
    "1. `metric_num_green_letters` counts the number of green tiles.\n",
    "1. `metric_letter_overlap` counts the number of distinct letters shared between the guess word and the solution word, regardless of their count.\n",
    "1. `metric_num_yellow_green` counts the number of tiles that are either yellow or green.\n",
    "1. `metric_hybrid` is a sum of `metric_num_green_letters` and `metric_num_yellow_green`. It's like `metric_num_yellow_green`, but it assigns 2 points to green tiles and 1 to yellow tiles, instead of equal score per tile.\n",
    "1. `metric_num_reduced_words` measures how many words out of all possible solutions are ruled out when playing this guess, based on the yellow/green tiles revealed.\n",
    "1. `metric_percent_reduced_word_pool` is exactly the same as `metric_num_reduced_words`, but represented as a percent of all possible solutions so that it's easier to interpret.\n",
    "\n",
    "The last two are much slower to compute than the rest, but are a more accurate representation of what we're looking for when searching for the best word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics ###\n",
    "\n",
    "def metric_num_green_letters(guess_word, actual_word):\n",
    "    return sum([x==y for x, y in zip(guess_word, actual_word)])\n",
    "\n",
    "def metric_letter_overlap(guess_word, actual_word):\n",
    "    return len(set(guess_word) & set(actual_word))\n",
    "\n",
    "def metric_num_yellow_green(guess_word, actual_word):\n",
    "    guess = list(guess_word)\n",
    "    actual = list(actual_word)\n",
    "    guess.sort()\n",
    "    actual.sort()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    result = 0\n",
    "    while i < len(guess) and j < len(actual):\n",
    "        if guess[i] == actual[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "            result += 1\n",
    "            continue\n",
    "        if guess[i] < actual[j]:\n",
    "            i += 1\n",
    "            continue\n",
    "        if guess[i] > actual[j]:\n",
    "            j += 1\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "def metric_hybrid(guess_word, actual_word):\n",
    "    return metric_num_green_letters(guess_word, actual_word) + metric_num_yellow_green(guess_word, actual_word)\n",
    "\n",
    "# After guessing a word, the game colors letters indicating restrictions on the final words.\n",
    "# This metric computes, on average, how many words are eliminated from the pool of possible solutions due to not meeting those restrictions.\n",
    "# The bigger this metric is, the smaller the word pool for the second guess, which would in theory translate to an easier guess.\n",
    "def metric_num_reduced_words(guess_word, actual_word):\n",
    "    if guess_word == actual_word:\n",
    "        return len(words_solutions) - 1\n",
    "    \n",
    "    is_green = [l == r for l, r in zip(guess_word, actual_word)]\n",
    "\n",
    "    guess_letters = set(guess_word)\n",
    "\n",
    "    count_requirements = {}\n",
    "    for letter in guess_letters:\n",
    "        guess_count = sum([x == letter for x in guess_word])\n",
    "        actual_count = sum([x == letter for x in actual_word])\n",
    "\n",
    "        green_count = sum([x == y for x, y in zip(guess_word, actual_word)])\n",
    "        yellow_count = min(actual_count - green_count, guess_count - green_count)\n",
    "        gray_count = guess_count - green_count - yellow_count\n",
    "\n",
    "        count_requirements[letter] = {\n",
    "            'min': green_count + yellow_count,\n",
    "            'max': (green_count + yellow_count) if gray_count > 0 else 5\n",
    "        }\n",
    "    \n",
    "    reduced_word_count = 0\n",
    "\n",
    "    for word in words_solutions:\n",
    "        # word is guess or solution. Don't filter out these words.\n",
    "        if word == guess_word or word == actual_word:\n",
    "            continue\n",
    "\n",
    "        # anything that isn't green indicates that the letter won't be in that spot in the solution, so make a filter for that.\n",
    "        if any((green != (letter == guess_letter) for green, letter, guess_letter in zip(is_green, word, guess_word))):\n",
    "            # green letter not in word. Or gray/yellow letter in word. Filter out.\n",
    "            reduced_word_count += 1\n",
    "            continue\n",
    "\n",
    "        break_early = False\n",
    "        for letter in set(word):\n",
    "            if break_early:\n",
    "                break\n",
    "            if letter in count_requirements:\n",
    "                letter_count = sum([letter == x for x in word])\n",
    "                counts_map = count_requirements[letter]\n",
    "                if letter_count < counts_map['min'] or letter_count > counts_map['max']:\n",
    "                    # min/max conditions not met. Filter out.\n",
    "                    reduced_word_count += 1\n",
    "                    break_early = True\n",
    "                    break\n",
    "        if break_early:\n",
    "            continue\n",
    "    \n",
    "    return reduced_word_count\n",
    "\n",
    "# This metric is similar to num_reduced_words, but it's given as a percentage to make it easier to interpret.\n",
    "def metric_percent_reduced_word_pool(guess_word, actual_word):\n",
    "    return 100 * metric_num_reduced_words(guess_word, actual_word) / (len(words_solutions) - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal word for each metric\n",
    "After having defined the metrics, we compute which out of all allowed words maximizes the metric. Note that these metric numbers can't be compared to each other, since it would be comparing \"apples to oranges\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('orate', 1.7892008639308856)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_word(metric_num_yellow_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saree', 0.6803455723542117)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_word(metric_num_green_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('orate', 1.7892008639308856)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_word(metric_letter_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('soare', 2.428077753779698)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_word(metric_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking these words with the most reliable metric\n",
    "The metric `metric_percent_reduced_word_pool`, which mentions on average what percent of the remaining word pool of possible answers becomes eliminated after playing a certain word, is prohibitively expensive to compute for all words. It can, however, be computed for a handful of words in a reasonable amount of time.\n",
    "\n",
    "This metric is in my opinion the most accurate of the ones mentioned in this blog post, since instead of using rough heuristics, it's getting at the actual thing that we want the starting word to do: reduce the number of remaining word possibilities the most.\n",
    "\n",
    "Below is the metric as computed for the metric-optimal words mentioned above. In this case, it *is* possible to compare the different numbers shown, since they're computed with the same metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.49661465285006"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score('orate', metric_percent_reduced_word_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.77083990584131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score('saree', metric_percent_reduced_word_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.55655219146912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score('soare', metric_percent_reduced_word_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best starting word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scores above, the winner for best starting words is **\"soare\"**! with a whopping 90.6% average reduction in the word pool size.\n",
    "\n",
    "It should be noted that it's very likely that there's a better word out there with a better score on the `percent_reduced_word_pool` metric, but because the metric is expensive to compute, it's not feasible to try all words on it, and it's necessary to start with heuristic metrics and then check that metric to see how it does, as was done in the above analysis.\n",
    "\n",
    "If you want to check how your favorite starting word compares to \"soare\" or the other metric-optimizing words seen above, simply run `compute_score('<your word here>', metric_percent_reduced_word_pool)` and you'll be able to see how well it scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue: Trying out other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.86302737959096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score('house', metric_percent_reduced_word_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.86370314229674"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score('earth', metric_percent_reduced_word_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.78441302915432"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score('ouija', metric_percent_reduced_word_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
